# ðŸ§¹ Historical Text Cleaning Process Flow

This document shows the **actual** text cleaning process implemented in `historical_data_collector.py`, not just theoretical cleaning steps.

> **ðŸ“Š Visual Flow**: See [Cleaning Flow Diagram](cleaning_flow_diagram.md) for a Mermaid diagram of the complete process.

## ðŸ“Š **Complete Cleaning Pipeline**

### **Phase 1: File Discovery & Initial Filtering**

```
ðŸ“ Raw Files
    â†“
ðŸ” File Type Detection
    â”œâ”€â”€ .txt, .txt.utf-8, _txt.utf-8 â†’ Text Processing
    â”œâ”€â”€ .pdf â†’ PDF Processing  
    â”œâ”€â”€ .html, .htm â†’ HTML Processing
    â”œâ”€â”€ .xml â†’ XML Processing (Old Bailey, London Lives)
    â””â”€â”€ No Extension â†’ Content Detection
        â”œâ”€â”€ HTML-like content â†’ HTML Processing
        â”œâ”€â”€ Text-like content â†’ Text Processing
        â””â”€â”€ Binary/Unknown â†’ REJECTED
    â†“
ðŸš« Filename Language Check
    â”œâ”€â”€ Non-English characters â†’ REJECTED (logged)
    â””â”€â”€ English/Latin â†’ Continue
```

### **Phase 2: Content Extraction by File Type**

#### **Text Files (.txt, .txt.utf-8, etc.)**
```
ðŸ“„ Text File
    â†“
ðŸ“– Read with UTF-8 encoding (errors='ignore')
    â†“
ðŸ§¹ clean_gutenberg_text()
    â”œâ”€â”€ Remove Project Gutenberg headers/footers
    â”œâ”€â”€ Remove metadata patterns
    â””â”€â”€ Clean whitespace
```

#### **PDF Files**
```
ðŸ“„ PDF File
    â†“
ðŸ”§ extract_text_from_pdf()
    â”œâ”€â”€ Try system pdftotext (preferred)
    â””â”€â”€ Fallback to PyPDF2
    â†“
ðŸ§¹ clean_pdf_text()
    â”œâ”€â”€ Remove page numbers
    â”œâ”€â”€ Remove library stamps
    â”œâ”€â”€ Remove headers/footers
    â””â”€â”€ Fix OCR artifacts (0â†’O, 1â†’I, 5â†’S, 8â†’B, rnâ†’m, clâ†’d, etc.)
```

#### **HTML Files**
```
ðŸ“„ HTML File
    â†“
ðŸ”§ clean_html_text()
    â”œâ”€â”€ BeautifulSoup parsing (if available)
    â”œâ”€â”€ Remove script, style, nav, header, footer
    â”œâ”€â”€ Extract text content
    â””â”€â”€ Remove wiki metadata
```

#### **XML Files (Old Bailey, London Lives)**
```
ðŸ“„ XML File
    â†“
ðŸ” Detect XML Type
    â”œâ”€â”€ Old Bailey XML â†’ extract_old_bailey_text()
    â”‚   â”œâ”€â”€ Extract trial accounts
    â”‚   â”œâ”€â”€ Extract front matter
    â”‚   â””â”€â”€ Preserve historical language
    â””â”€â”€ London Lives XML â†’ extract_london_lives_text()
        â”œâ”€â”€ Extract paragraphs with semantic markup
        â”œâ”€â”€ Extract lists
        â””â”€â”€ Preserve person names, places, occupations
    â†“
ðŸ§¹ Type-specific cleaning
    â”œâ”€â”€ Old Bailey â†’ clean_old_bailey_text()
    â””â”€â”€ London Lives â†’ clean_london_lives_text()
```

### **Phase 3: Text Normalization**

```
ðŸ“ Extracted Text
    â†“
ðŸ”§ normalize_text()
    â”œâ”€â”€ Fix encoding issues (Ã¢â‚¬â„¢â†’', Ã¢â‚¬Å“â†’", Ã¢â‚¬"â†’â€”, etc.)
    â”œâ”€â”€ Normalize Unicode (NFC)
    â”œâ”€â”€ Handle single long lines â†’ break_long_line()
    â”œâ”€â”€ Normalize line endings (\r\nâ†’\n)
    â””â”€â”€ Clean excessive whitespace
```

### **Phase 4: Quality Validation**

```
ðŸ“ Normalized Text
    â†“
ðŸ” Duplicate Detection
    â”œâ”€â”€ Content hash check â†’ REJECTED if duplicate
    â””â”€â”€ Continue if unique
    â†“
ðŸŒ Language Detection
    â”œâ”€â”€ Non-English detected â†’ REJECTED (logged)
    â””â”€â”€ English â†’ Continue
    â†“
ðŸ“Š Quality Analysis (analyze_text_quality())
    â”œâ”€â”€ Length checks (min 200 chars)
    â”œâ”€â”€ Project Gutenberg validation (relaxed criteria)
    â”œâ”€â”€ Historical text validation (very relaxed)
    â”œâ”€â”€ OCR artifact detection
    â”œâ”€â”€ Advertisement density check
    â””â”€â”€ Meaningful word ratio (â‰¥50%)
    â†“
âŒ Poor Quality â†’ REJECTED (logged with details)
âœ… Good Quality â†’ Continue
```

### **Phase 5: Final Processing**

```
âœ… Validated Text
    â†“
ðŸ’¾ Save to Processed Directory
    â”œâ”€â”€ Filename: cleaned_{original_stem}.txt
    â”œâ”€â”€ UTF-8 encoding
    â””â”€â”€ Update statistics
    â†“
ðŸ“Š Statistics Tracking
    â”œâ”€â”€ Characters before/after
    â”œâ”€â”€ Files processed/cleaned/skipped/failed
    â”œâ”€â”€ OCR artifacts fixed
    â”œâ”€â”€ Gutenberg headers removed
    â””â”€â”€ Rejection reasons logged
```

## ðŸ”§ **Detailed Cleaning Functions**

### **Project Gutenberg Cleaning**
```python
def clean_gutenberg_text(text: str) -> str:
    # Remove START/END markers
    # Remove metadata patterns (Title, Author, Release Date, etc.)
    # Clean excessive whitespace
    # Preserve historical language patterns
```

### **PDF Cleaning**
```python
def clean_pdf_text(text: str) -> str:
    # Remove page numbers: [Page 123], Page 123, standalone numbers
    # Remove library stamps: Internet Archive, Google, etc.
    # Remove headers/footers: all-caps lines, chapter numbers
    # Fix OCR errors: 0â†’O, 1â†’I, 5â†’S, 8â†’B, rnâ†’m, clâ†’d, iiâ†’n, vvâ†’w, Å¿â†’s
```

### **HTML Cleaning**
```python
def clean_html_text(html_content: str) -> str:
    # BeautifulSoup parsing (if available)
    # Remove unwanted elements: script, style, nav, header, footer, aside
    # Extract text content with proper spacing
    # Remove wiki metadata: "This page was last edited", "Jump to navigation", etc.
```

### **XML Cleaning (Old Bailey)**
```python
def extract_old_bailey_text(soup) -> str:
    # Extract trial accounts (main narrative)
    # Extract front matter (session info)
    # Preserve historical language and structure
    # Clean up XML artifacts while maintaining readability
```

### **XML Cleaning (London Lives)**
```python
def extract_london_lives_text(soup) -> str:
    # Extract paragraphs with semantic markup
    # Preserve person names, places, occupations, dates
    # Extract lists with proper formatting
    # Clean up markup artifacts
```

## ðŸ“ˆ **Quality Analysis Details**

### **OCR Artifact Detection**
- **Long capitals**: `[A-Z]{5,}\s+[A-Z]{5,}`
- **Spaced letters**: `\b[A-Za-z]\s+[A-Za-z]\s+[A-Za-z]\s+[A-Za-z]\b`
- **Special characters**: `[!@#$%^&*()]{3,}`
- **Mixed numbers/letters**: `\b\d+[A-Za-z]+\d+\b`
- **Long non-word sequences**: `[^\w\s]{10,}`

### **Advertisement Detection**
- **Common patterns**: "this day is published", "just ready", "elegantly bound"
- **Price notations**: "price \d+s"
- **Publisher references**: "paternoster row", "corner of", "publishers"
- **Book advertisements**: "now ready", "new novels", "advertisements"

### **Meaningful Word Analysis**
- **Ratio calculation**: meaningful_words / total_words
- **Threshold**: â‰¥50% for general content
- **Relaxed criteria**: â‰¥40% for Project Gutenberg
- **Very relaxed**: â‰¥1000 chars + 100 words for historical texts

## ðŸš« **Rejection Reasons & Logging**

### **Automatic Rejections**
1. **Non-English filename** (Arabic, Chinese, Cyrillic, etc.)
2. **Duplicate content** (hash-based detection)
3. **Non-English content** (language detection)
4. **Poor quality** (OCR artifacts, ads, low meaningful word ratio)
5. **Too short** (<200 characters, <5 lines, <50 words)
6. **Unsupported file type** (binary, unknown format)

### **Rejection Logging**
```json
{
  "timestamp": "2024-01-15 10:30:45",
  "file_path": "/path/to/file.txt",
  "filename": "file.txt",
  "file_size": 1234,
  "rejection_reason": "Poor content quality",
  "details": {
    "text_length": 150,
    "rejection_reasons": ["Text too short (< 200 chars)"],
    "ocr_issues": [],
    "advertisement_indicators": [],
    "meaningful_word_ratio": 0.3
  },
  "preview": "First 500 characters of file content"
}
```

## ðŸ“Š **Statistics Tracking**

### **Processing Statistics**
- **Files**: downloaded, processed, cleaned, skipped, failed, sanitized
- **Characters**: before cleaning, after cleaning, removed
- **Quality**: Gutenberg headers removed, OCR artifacts fixed, HTML markup removed
- **Content**: duplicates found, non-English skipped, poor quality skipped
- **Sources**: Gutenberg processed/accepted, encoding issues fixed

### **Rejection Statistics**
- **Total rejected files** with detailed reasons
- **Rejection summary** by reason type
- **File previews** for manual review
- **Quality analysis details** for each rejection

## ðŸ”„ **Corpus Creation Process**

```
ðŸ“ Cleaned Files
    â†“
ðŸ”§ create_comprehensive_corpus()
    â”œâ”€â”€ Read all cleaned_*.txt files
    â”œâ”€â”€ Split into training segments (split_into_training_segments)
    â”‚   â”œâ”€â”€ Split on double newlines (paragraphs)
    â”‚   â”œâ”€â”€ Max length: 2000 characters
    â”‚   â”œâ”€â”€ Min length: 100 characters
    â”‚   â””â”€â”€ Further split long segments at sentence boundaries
    â”œâ”€â”€ Filter segments (min 50 characters)
    â””â”€â”€ Write to london_historical_corpus_comprehensive.txt
```

## ðŸŽ¯ **Key Differences from TEXT_CLEANING_GUIDE.md**

### **What's Actually Implemented**
- âœ… **File type detection** and routing
- âœ… **Format-specific cleaning** (PDF, HTML, XML)
- âœ… **Historical text preservation** (Old Bailey, London Lives)
- âœ… **Quality analysis** with detailed rejection logging
- âœ… **Duplicate detection** using content hashing
- âœ… **Language detection** and filtering
- âœ… **OCR artifact detection** and correction
- âœ… **Advertisement detection** and filtering
- âœ… **Corpus segmentation** for training

### **What's Not Implemented**
- âŒ **Separate text_cleaner.py** script (functionality integrated)
- âŒ **Quality scoring system** (0-100 points)
- âŒ **Batch processing** with separate scripts
- âŒ **Advanced OCR correction** patterns
- âŒ **Separate analysis tools**

## ðŸ”— **Integration Points**

### **Called From**
- `historical_data_collector.py` â†’ `process_file()` method
- `download_and_process_sources()` â†’ Main collection pipeline
- `create_comprehensive_corpus()` â†’ Final corpus creation

### **Configuration**
- **Data source flags** in `config.py` (enable_old_bailey, enable_london_lives, etc.)
- **Quality thresholds** hardcoded in analysis functions
- **File type detection** based on extensions and content

---

**This is the actual cleaning process implemented in the codebase, not the theoretical process described in TEXT_CLEANING_GUIDE.md.**
